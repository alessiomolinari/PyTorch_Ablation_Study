{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Study with PyTorch\n",
    "## Titanic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class Ablator:\n",
    "    def __init__(self, model, dataset, dataloader_kwargs, training_fn):\n",
    "        # TODO maybe you can have a check that model must be a nn.Sequential, otherwise you make it sequential\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        # TODO dataset_features might call it ablation space? or maybe it can be included in Maggy Dataset?\n",
    "        # Maybe not necessary at all?\n",
    "        # self.dataset_features = dataset_features\n",
    "        self.dataloader_kwargs = dataloader_kwargs\n",
    "        self.training_fn = training_fn\n",
    "\n",
    "        self.trials = []\n",
    "        self.state_dictionary = model.state_dict()\n",
    "\n",
    "    def ablate_layers(self, idx_list, input_shape, infer_activation=False):\n",
    "        if idx_list is None:\n",
    "            return copy.deepcopy(self.model)\n",
    "            # Why a copy? Because if you perform a multiple feature ablation without layer ablation you train on the\n",
    "            #  same model over and over again.\n",
    "        if type(idx_list) == int:\n",
    "            idx_list = [idx_list]\n",
    "        elif type(idx_list) != list:\n",
    "            raise TypeError(\"idx_to_ablate should be an integer or a list of integers\")\n",
    "\n",
    "        new_modules = self._get_module_list()\n",
    "\n",
    "        if infer_activation:\n",
    "            activations_idx = []\n",
    "            for idx in idx_list:\n",
    "                if ((idx + 1) < len(new_modules)) and self._is_activation(new_modules[idx + 1]):\n",
    "                    activations_idx.append(idx + 1)\n",
    "            idx_list = idx_list + activations_idx\n",
    "            idx_list = list(set(idx_list))\n",
    "\n",
    "        ablated_modules = self.remove_modules(new_modules, idx_list)\n",
    "        correct_modules = self._match_model_features(ablated_modules, input_shape)\n",
    "        ablated_model = nn.Sequential(*correct_modules)\n",
    "\n",
    "        return ablated_model\n",
    "\n",
    "    @staticmethod\n",
    "    def _match_model_features(model_modules, input_shape):\n",
    "        # TODO you have to do a lot of testing with different pytorch layers\n",
    "        tensor_shape = (1,) + input_shape\n",
    "        last_valid_out_features = tensor_shape[1]\n",
    "        i = 0\n",
    "        input_tensor = torch.rand(tensor_shape)\n",
    "        anti_stuck_idx = 0\n",
    "\n",
    "        while i < len(model_modules):\n",
    "            layer = model_modules[i]\n",
    "\n",
    "            try:\n",
    "                output_tensor = layer(input_tensor)\n",
    "                anti_stuck_idx = 0\n",
    "                last_valid_out_features = output_tensor.shape[1]\n",
    "                # print(layer, \"\\t\\t\", output_tensor.shape)\n",
    "                i += 1\n",
    "                input_tensor = output_tensor\n",
    "\n",
    "            except RuntimeError:\n",
    "                anti_stuck_idx += 1\n",
    "\n",
    "                if anti_stuck_idx > 1:\n",
    "                    raise RuntimeError(\"Ablation failed. Check again what modules you are ablating\")\n",
    "\n",
    "                layer_type = type(layer)\n",
    "                layer_signature = inspect.signature(layer_type)\n",
    "                parameters = dir(layer) & layer_signature.parameters.keys()\n",
    "                new_args = dict()\n",
    "\n",
    "                for key, value in layer.__dict__.items():\n",
    "                    if key in parameters:\n",
    "                        new_args[key] = value\n",
    "\n",
    "                if \"in_features\" in new_args:\n",
    "                    new_args[\"in_features\"] = last_valid_out_features\n",
    "\n",
    "                elif \"in_channels\" in new_args:\n",
    "                    new_args[\"in_channels\"] = last_valid_out_features\n",
    "\n",
    "                # This new initialization is necessary because even if you change the shape of the layer,\n",
    "                #  without initialization you don't have the correct number of weights\n",
    "                model_modules[i] = layer_type(**new_args)\n",
    "        return model_modules\n",
    "\n",
    "    def new_trial(self, input_shape, ablated_layers=None, ablated_features=None, infer_activation=False):\n",
    "        # TODO you don't really need the whole input shape but just the initial features actually\n",
    "        self.trials.append(Trial(input_shape, ablated_layers, ablated_features, infer_activation))\n",
    "\n",
    "    def execute_trials(self):\n",
    "        for i, trial in enumerate(self.trials):\n",
    "            print(\"Starting trial\", i)\n",
    "\n",
    "            original_data = self.dataset.data\n",
    "\n",
    "            # 1) Ablate layers\n",
    "            ablated_model = self.ablate_layers(trial.ablated_layers, trial.input_shape, trial.infer_activation)\n",
    "\n",
    "            # 2) Ablate features\n",
    "            if trial.ablated_features is not None:\n",
    "                print(\"Ablating features:\", trial.ablated_features)\n",
    "                self.dataset.ablate_feature(trial.ablated_features)\n",
    "\n",
    "            # 3) Match features in model\n",
    "            self._match_model_features(ablated_model, trial.input_shape)\n",
    "\n",
    "            # 4) Train\n",
    "            dataloader = DataLoader(self.dataset, **self.dataloader_kwargs)\n",
    "            trial.metric = self.training_fn(ablated_model, dataloader)\n",
    "            print(\"Final metric:\", trial.metric, \"\\n\\n\")\n",
    "\n",
    "            # 5) Restore original data\n",
    "            self.dataset.data = original_data\n",
    "\n",
    "    def _get_module_list(self):\n",
    "        modules = []\n",
    "        for mod in self.model.modules():\n",
    "            # TODO this is just a quick patch but you should find a better solution\n",
    "            if not str(mod).startswith(\"Sequential\"):\n",
    "                modules.append(mod)\n",
    "        # In PyTorch the first module is actually a description of the whole model\n",
    "        modules.pop(0)\n",
    "        return modules\n",
    "\n",
    "    def remove_modules(self, modules_list, modules_to_ablate):\n",
    "        for i in reversed(sorted(modules_to_ablate)):\n",
    "            self._ablate_and_print(modules_list, i)\n",
    "        return modules_list\n",
    "\n",
    "    @staticmethod\n",
    "    def _ablate_and_print(modules, i):\n",
    "        ablated = modules.pop(i)\n",
    "        print(\"Ablating \", i, \" - \", ablated, sep=\"\")\n",
    "\n",
    "    # TODO static methods might be probably refactored to a module utils.py\n",
    "    @staticmethod\n",
    "    def _is_activation(layer):\n",
    "        from torch.nn.modules import activation\n",
    "        activation_functions = inspect.getmembers(activation, inspect.isclass)\n",
    "        activation_functions = [x[0] for x in activation_functions]\n",
    "        if layer.__class__.__name__ in activation_functions:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "class MaggyDataset:\n",
    "    \"\"\"\n",
    "    In PyTorch there is no way to get the entire dataset starting from the classes Dataset or DataLoader.\n",
    "     This is because the only method whose implementation is guaranteed is __getitem__ (enumerate) but there is\n",
    "     no specification on what this method should return. For instance, it could return a row of a tabular dataset,\n",
    "     as well as a tuple (label, row). For this reason we necessitate a method that returns a tabular dataset\n",
    "    (tabular because we define feature ablation only on tabular datasets for now) on which we can ablate the columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def ablate_feature(self, feature):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Trial:\n",
    "    def __init__(self, input_shape, ablated_layers, ablated_features, infer_activation):\n",
    "        self.ablated_layers = ablated_layers\n",
    "        self.ablated_features = ablated_features\n",
    "        self.input_shape = input_shape\n",
    "        self.infer_activation = infer_activation\n",
    "        self.metric = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import inspect\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../titanic_train.csv\"\n",
    "test_path = \"../titanic_test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "(418, 11)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1       0.0       3   \n",
       "1            2       1.0       1   \n",
       "2            3       1.0       3   \n",
       "3            4       1.0       1   \n",
       "4            5       0.0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join datasets\n",
    "\n",
    "all_df = pd.concat([train, test])\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId                            51\n",
       "Survived                                0\n",
       "Pclass                                  3\n",
       "Name           Panula, Master. Juha Niilo\n",
       "Sex                                  male\n",
       "Age                                     7\n",
       "SibSp                                   4\n",
       "Parch                                   1\n",
       "Ticket                            3101295\n",
       "Fare                              39.6875\n",
       "Cabin                                 NaN\n",
       "Embarked                                S\n",
       "Name: 50, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the test dataset \"Survived\" is NaN\n",
    "\n",
    "all_df.iloc[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      int64\n",
      "Survived       float64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# We can see that the last column is type object so we cast it to string\n",
    "\n",
    "all_df['Embarked'] = all_df['Embarked'].astype(str)\n",
    "print(all_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "\n",
    "all_df = all_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional info for dataset ablation\n",
    "\n",
    "dataset_features = list(all_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "\n",
    "cat_cols = ['Pclass', 'Sex', 'SibSp', 'Parch']\n",
    "for col in cat_cols:\n",
    "    all_df[col] = LabelEncoder().fit_transform(all_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you wanted to consider feature Embarked too\n",
    "\n",
    "#all_df['Embarked'] = LabelEncoder().fit_transform(all_df['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs\n",
    "\n",
    "all_df = all_df.fillna(all_df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "\n",
    "import sklearn.preprocessing\n",
    "\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "all_df = min_max_scaler.fit_transform(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train/test again\n",
    "\n",
    "train_df = all_df[:train.shape[0]]\n",
    "test_df = all_df[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, there are two important classes for data handling: Dataset and DataLoader  \n",
    "\n",
    "**Dataset** is the parent class from which your own dataset class inherits. It keeps the data in a numpy format and overrides two methods \\_\\_getitem\\_\\_ and \\_\\_len\\_\\_ (len is optional) that will be called when you iterate through the dataset.  \n",
    "\n",
    "**DataLoader** is the class that is used to iterate through the dataset and feed it to your model. You need to specify some parameters during creation like the dataset itself and hyperparameters such as batch_size, shuffle, num_workers, etc.\n",
    "\n",
    "For this class *TitanicDataset* we add two more requirements that stems from inheriting *MaggyDataset*:\n",
    "1. All the data should be saved under the attribute `data` of your dataset\n",
    "2. The dataset class should override the method `ablate_feature` for every possible feature to ablate. This method shouldn't return anything: it should just eliminate a feature from `dataset.data`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset, MaggyDataset):\n",
    "    def __init__(self, data):\n",
    "        super(TitanicDataset).__init__()\n",
    "        self.data = data\n",
    "\n",
    "    # Optional override\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    # Compulsory override\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def ablate_feature(self, feature):\n",
    "        # The user defines this function that explains the Ablator how to ablate a specific feature.\n",
    "        # Not just columns can be ablated but even image channels and anything that is defined in the ablation space.\n",
    "        global dataset_features\n",
    "        idx = dataset_features.index(feature)\n",
    "        self.data = np.delete(self.data, idx, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_kwargs = {\"batch_size\":64, \"shuffle\":False}\n",
    "\n",
    "dataset = TitanicDataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(6, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 32),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(32, 32),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(32, 2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(2, 1),\n",
    "                      nn.Sigmoid()\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def training_function(model, dataloader):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    training_history = []\n",
    "    epochs = 5\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for i, data in enumerate(dataloader):\n",
    "            # print(\"load number {}\".format(i))\n",
    "\n",
    "            inputs = data[:, 1:]\n",
    "            labels = data[:, 0]\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float().view(-1, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(inputs)\n",
    "\n",
    "            loss = criterion(y_pred, labels)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            accuracy = ((y_pred > 0.5).float() == labels).float().mean()\n",
    "            epoch_accuracy += accuracy\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Epoch:{}, Loss:{}, Accuracy:{}\".format(epoch,\n",
    "                                                      epoch_loss.item(),\n",
    "                                                      epoch_accuracy / dataloader.batch_size))\n",
    "        training_history.append(epoch_loss.item() / len(train_df))\n",
    "    print(\"Training complete\\n\")\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 0\n",
      "Epoch:0, Loss:9.455788612365723, Accuracy:0.1348194181919098\n",
      "Epoch:1, Loss:9.439217567443848, Accuracy:0.1348194181919098\n",
      "Epoch:2, Loss:9.42446517944336, Accuracy:0.1348194181919098\n",
      "Epoch:3, Loss:9.411325454711914, Accuracy:0.1348194181919098\n",
      "Epoch:4, Loss:9.399618148803711, Accuracy:0.1348194181919098\n",
      "Training complete\n",
      "\n",
      "Final metric: tensor(0.6634, grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "\n",
      "Starting trial 1\n",
      "Ablating features: Sex\n",
      "Epoch:0, Loss:9.459930419921875, Accuracy:0.1348194181919098\n",
      "Epoch:1, Loss:9.443719863891602, Accuracy:0.1348194181919098\n",
      "Epoch:2, Loss:9.429322242736816, Accuracy:0.1348194181919098\n",
      "Epoch:3, Loss:9.416543960571289, Accuracy:0.1348194181919098\n",
      "Epoch:4, Loss:9.405226707458496, Accuracy:0.1348194181919098\n",
      "Training complete\n",
      "\n",
      "Final metric: tensor(0.6641, grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "\n",
      "Starting trial 2\n",
      "Ablating 3 - Linear(in_features=64, out_features=32, bias=True)\n",
      "Ablating 2 - ReLU()\n",
      "Epoch:0, Loss:9.388978958129883, Accuracy:0.1348194181919098\n",
      "Epoch:1, Loss:9.34975528717041, Accuracy:0.1348194181919098\n",
      "Epoch:2, Loss:9.312884330749512, Accuracy:0.1348194181919098\n",
      "Epoch:3, Loss:9.27655029296875, Accuracy:0.1348194181919098\n",
      "Epoch:4, Loss:9.241517066955566, Accuracy:0.1348194181919098\n",
      "Training complete\n",
      "\n",
      "Final metric: tensor(0.6482, grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "\n",
      "Starting trial 3\n",
      "Ablating 4 - ReLU()\n",
      "Ablating features: Pclass\n",
      "Epoch:0, Loss:9.444869041442871, Accuracy:0.1348194181919098\n",
      "Epoch:1, Loss:9.423369407653809, Accuracy:0.1348194181919098\n",
      "Epoch:2, Loss:9.403899192810059, Accuracy:0.1348194181919098\n",
      "Epoch:3, Loss:9.386241912841797, Accuracy:0.1348194181919098\n",
      "Epoch:4, Loss:9.370713233947754, Accuracy:0.1348194181919098\n",
      "Training complete\n",
      "\n",
      "Final metric: tensor(0.6601, grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "\n",
      "Starting trial 4\n",
      "Ablating 6 - ReLU()\n",
      "Ablating 5 - Linear(in_features=32, out_features=32, bias=True)\n",
      "Ablating 4 - ReLU()\n",
      "Ablating 3 - Linear(in_features=64, out_features=32, bias=True)\n",
      "Epoch:0, Loss:9.367860794067383, Accuracy:0.1348194181919098\n",
      "Epoch:1, Loss:9.275850296020508, Accuracy:0.1348194181919098\n",
      "Epoch:2, Loss:9.1732177734375, Accuracy:0.1348194181919098\n",
      "Epoch:3, Loss:9.060127258300781, Accuracy:0.1348194181919098\n",
      "Epoch:4, Loss:8.962355613708496, Accuracy:0.1348194181919098\n",
      "Training complete\n",
      "\n",
      "Final metric: tensor(0.6154, grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ablator = Ablator(model, dataset, dataloader_kwargs, training_function)\n",
    "\n",
    "ablator.new_trial((6,), None, None)\n",
    "ablator.new_trial((5,), None, \"Sex\")\n",
    "ablator.new_trial((6,), 2, None, infer_activation=True)\n",
    "ablator.new_trial((5,), 4, \"Pclass\")\n",
    "ablator.new_trial((6,), [3, 4, 5, 6], None)\n",
    "\n",
    "ablator.execute_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai]",
   "language": "python",
   "name": "conda-env-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
