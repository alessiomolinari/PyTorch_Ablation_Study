{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Ablation Study\n",
    "## Feature ablation\n",
    "You should be able to pip install this package from GitHub but I still haven't understood why it doesn't let you do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class Ablator:\n",
    "    def __init__(self, model, dataset, dataset_features, dataloader_kwargs, training_fn):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.dataset_features = dataset_features\n",
    "        self.dataloader_kwargs = dataloader_kwargs\n",
    "        self.training_fn = training_fn\n",
    "\n",
    "        self.trials = []\n",
    "        self.state_dictionary = model.state_dict()\n",
    "\n",
    "    def ablate_layers(self, idx_list, input_shape, infer_activation=False):\n",
    "        if idx_list is None:\n",
    "            return copy.deepcopy(self.model)\n",
    "            # Why a copy? Because if you perform a multiple feature ablation without layer ablation you train on the\n",
    "            #  same model over and over again.\n",
    "        if type(idx_list) == int:\n",
    "            idx_list = [idx_list]\n",
    "        elif type(idx_list) != list:\n",
    "            raise TypeError(\"idx_to_ablate should be an integer or a list of integers\")\n",
    "\n",
    "        new_modules = self._get_module_list()\n",
    "\n",
    "        if infer_activation:\n",
    "            activations_idx = []\n",
    "            for idx in idx_list:\n",
    "                if ((idx + 1) < len(new_modules)) and self._is_activation(new_modules[idx + 1]):\n",
    "                    activations_idx.append(idx + 1)\n",
    "            idx_list = idx_list + activations_idx\n",
    "            idx_list = list(set(idx_list))\n",
    "\n",
    "        ablated_modules = self.remove_modules(new_modules, idx_list)\n",
    "        correct_modules = self._match_model_features(ablated_modules, input_shape)\n",
    "        ablated_model = nn.Sequential(*correct_modules)\n",
    "\n",
    "        return ablated_model\n",
    "\n",
    "    @staticmethod\n",
    "    def _match_model_features(model_modules, input_shape):\n",
    "        tensor_shape = (1,) + input_shape\n",
    "        last_valid_out_features = tensor_shape[1]\n",
    "        i = 0\n",
    "        input_tensor = torch.rand(tensor_shape)\n",
    "        anti_stuck_idx = 0\n",
    "\n",
    "        while i < len(model_modules):\n",
    "            layer = model_modules[i]\n",
    "\n",
    "            try:\n",
    "                output_tensor = layer(input_tensor)\n",
    "                anti_stuck_idx = 0\n",
    "                last_valid_out_features = output_tensor.shape[1]\n",
    "                # print(layer, \"\\t\\t\", output_tensor.shape)\n",
    "                i += 1\n",
    "                input_tensor = output_tensor\n",
    "\n",
    "            except RuntimeError:\n",
    "                anti_stuck_idx += 1\n",
    "\n",
    "                if anti_stuck_idx > 1:\n",
    "                    raise RuntimeError(\"Ablation failed. Check again what modules you are ablating\")\n",
    "\n",
    "                layer_type = type(layer)\n",
    "                layer_signature = inspect.signature(layer_type)\n",
    "                parameters = dir(layer) & layer_signature.parameters.keys()\n",
    "                new_args = dict()\n",
    "\n",
    "                for key, value in layer.__dict__.items():\n",
    "                    if key in parameters:\n",
    "                        new_args[key] = value\n",
    "\n",
    "                if \"in_features\" in new_args:\n",
    "                    new_args[\"in_features\"] = last_valid_out_features\n",
    "\n",
    "                elif \"in_channels\" in new_args:\n",
    "                    new_args[\"in_channels\"] = last_valid_out_features\n",
    "\n",
    "                # This new initialization is necessary because even if you change the shape of the layer,\n",
    "                #  without initialization you don't have the correct number of weights\n",
    "                model_modules[i] = layer_type(**new_args)\n",
    "        return model_modules\n",
    "\n",
    "    def new_trial(self, input_shape, ablated_layers=None, ablated_features=None, infer_activation=False):\n",
    "        self.trials.append(Trial(input_shape, ablated_layers, ablated_features, infer_activation))\n",
    "\n",
    "    def execute_trials(self):\n",
    "        for i, trial in enumerate(self.trials):\n",
    "            print(\"Starting trial\", i)\n",
    "\n",
    "            original_data = self.dataset.data\n",
    "\n",
    "            # 1) Ablate layers\n",
    "            ablated_model = self.ablate_layers(trial.ablated_layers, trial.input_shape, trial.infer_activation)\n",
    "\n",
    "            # 2) Ablate features\n",
    "            if trial.ablated_features is not None:\n",
    "                print(\"Ablating features:\", trial.ablated_features)\n",
    "                self.dataset.ablate_feature(trial.ablated_features)\n",
    "\n",
    "            # 3) Match features in model\n",
    "            self._match_model_features(ablated_model, trial.input_shape)\n",
    "\n",
    "            # 4) Train\n",
    "            dataloader = DataLoader(self.dataset, **self.dataloader_kwargs)\n",
    "            trial.metric = self.training_fn(ablated_model, dataloader)\n",
    "            print(\"Final metric:\", trial.metric, \"\\n\\n\")\n",
    "\n",
    "            # 5) Restore original data\n",
    "            self.dataset.data = original_data\n",
    "\n",
    "    def _get_module_list(self):\n",
    "        modules = []\n",
    "        for mod in self.model.modules():\n",
    "            modules.append(mod)\n",
    "        # In PyTorch the first module is actually a description of the whole model\n",
    "        modules.pop(0)\n",
    "        return modules\n",
    "\n",
    "    def remove_modules(self, modules_list, modules_to_ablate):\n",
    "        for i in reversed(sorted(modules_to_ablate)):\n",
    "            self._ablate_and_print(modules_list, i)\n",
    "        return modules_list\n",
    "\n",
    "    @staticmethod\n",
    "    def _ablate_and_print(modules, i):\n",
    "        ablated = modules.pop(i)\n",
    "        print(\"Ablating \", i, \" - \", ablated, sep=\"\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_activation(layer):\n",
    "        from torch.nn.modules import activation\n",
    "        activation_functions = inspect.getmembers(activation, inspect.isclass)\n",
    "        activation_functions = [x[0] for x in activation_functions]\n",
    "        if layer.__class__.__name__ in activation_functions:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "class MaggyDataset:\n",
    "    \"\"\"\n",
    "    In PyTorch there is no way to get the entire dataset starting from the classes Dataset or DataLoader.\n",
    "     This is because the only method whose implementation is guaranteed is __getitem__ (enumerate) but there is\n",
    "     no specification on what this method should return. For instance, it could return a row of a tabular dataset,\n",
    "     as well as a tuple (label, row). For this reason we necessitate a method that returns a tabular dataset\n",
    "    (tabular because we define feature ablation only on tabular datasets for now) on which we can ablate the columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def ablate_feature(self, feature):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Trial:\n",
    "    def __init__(self, input_shape, ablated_layers, ablated_features, infer_activation):\n",
    "        self.ablated_layers = ablated_layers\n",
    "        self.ablated_features = ablated_features\n",
    "        self.input_shape = input_shape\n",
    "        self.infer_activation = infer_activation\n",
    "        self.metric = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not detect requirement name for 'git+https://github.com/alessiomolinari/PyTorch_Ablation_Study@master', please specify one with #egg=your_package_name\n"
     ]
    }
   ],
   "source": [
    "# This should work but then I can't import anything\n",
    "\n",
    "!pip install -e git+https://github.com/alessiomolinari/PyTorch_Ablation_Study#egg=Pytorch_Ablation_Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_ablation import ablator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here starts the demo\n",
    "\n",
    "For this demo we are going to use the CIFAR10 dataset with a PyTorch sequential model. Of course in a classical feature ablation study it's quite common to eliminate one or more columns from a dataset and retrain the model to see if the performance changes.  \n",
    "What if the dataset is not tabular? We could deal with images or time series and necessitate more refined ablation.  \n",
    "As PyTorch allows the user a lot of freedom in defining models and datasets, this framework follows the same philosophy and lets the user decide how to ablate her own features.  \n",
    "<br>\n",
    "**Step 1: Get your dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from pytorch_ablation.ablator import Ablator, MaggyDataset\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar_train = datasets.cifar.CIFAR10(root=\"data\", train=True, download=True, transform=transform_train)\n",
    "cifar_test = datasets.cifar.CIFAR10(root=\"data\", train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the requirements of the framework is to put your dataloader arguments in a dictionary rather than building the actual dataloader.  \n",
    "This is necessary because different dataloaders will be dynamically built after the feature ablation trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_kwargs = {\"batch_size\": 128, \"shuffle\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Build your model**  \n",
    "<br>\n",
    "At the moment the framework supports only PyTorch Sequential models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Conv2d(3, 5, 3, 1),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Conv2d(5,7, 3, 1),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Conv2d(7, 10, 3, 1),\n",
    "                     nn.ReLU(),\n",
    "                     nn.MaxPool2d(2),\n",
    "                     nn.Dropout2d(0.25),\n",
    "                     nn.Flatten(),\n",
    "                     nn.Linear(1000, 128),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Dropout(0.5),\n",
    "                     nn.Linear(128, 10),\n",
    "                     nn.LogSoftmax(dim=1)\n",
    "                     )\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "device = \"cpu\"\n",
    "epochs = 3\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Define your training function**  \n",
    "<br>\n",
    "The first and second argument of the training function should be:\n",
    "1. Model  \n",
    "2. Dataloader \n",
    "\n",
    "While the **returned object** is the metric that you want to store when a trial is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                          momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: define your ablation**  \n",
    "A MaggyDataset has two requirements:\n",
    "1. All data should be stored in the attribute data of your PyTorch dataset\n",
    "2. For every possible feature ablation you should define how you actually ablate this data (i.e. override the method ablate_feature)\n",
    "\n",
    "For instance, in this example we have CIFAR10 and we define how to ablate the first and the second channel.  \n",
    "Since the dataset was just loaded from the package torchvision and we have no access to the code of the class Dataset we need to define the function *ablate_feature* and manually attach it to CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom ablation\n",
    "def ablate_feature(self, feature):\n",
    "    if feature == \"ch1\":\n",
    "        self.data = np.delete(self.data, obj=0, axis=3)\n",
    "    if feature == \"ch2\":\n",
    "        self.data = np.delete(self.data, obj=1, axis=3)\n",
    "\n",
    "\n",
    "# In this case you have to attach the method to the dataset because we have no access to the dataset class code\n",
    "\n",
    "setattr(datasets.cifar.CIFAR10, \"ablate_feature\", ablate_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ablate_feature() missing 2 required positional arguments: 'self' and 'feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a18a1f238791>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Why doesn't this work?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcifar_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mablate_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: ablate_feature() missing 2 required positional arguments: 'self' and 'feature'"
     ]
    }
   ],
   "source": [
    "# Why doesn't this work?\n",
    "\n",
    "cifar_train.ablate_feature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last thing: define your trials**\n",
    "A trial is in the form (input_shape, ablated_layers, ablate_features)  \n",
    "<br>\n",
    "*input_shape* is the shape of the tensor that you input your network withouth considering the batch dimension.  \n",
    "*ablated_layers* is an integer or a list of integers representing the indeces of the layers that you want to ablate.  \n",
    "*ablated_feature* is the feature that you want to ablate from your dataset. For now only one feature per trial is supported.  \n",
    "<br>\n",
    "Instantiate your ablator, add the trials and then run them with:\n",
    "```\n",
    "ablator.execute_trials()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 0\n",
      "Ablating features: ch1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ablate_feature() missing 1 required positional argument: 'feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-008402cc3587>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mablator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mablator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_trials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\allem\\desktop\\python notebooks\\thesis notebooks\\src\\pytorch-ablation-study\\pytorch_ablation\\ablator.py\u001b[0m in \u001b[0;36mexecute_trials\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mablated_features\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Ablating features:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mablated_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mablate_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mablated_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# 3) Match features in model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ablate_feature() missing 1 required positional argument: 'feature'"
     ]
    }
   ],
   "source": [
    "ablator = Ablator(model, cifar_train, dataloader_kwargs, train)\n",
    "\n",
    "# ablator.new_trial((3, 32, 32), None, None)\n",
    "ablator.new_trial((2, 32, 32), None, \"ch1\")\n",
    "ablator.new_trial((3, 32, 32), 2, None)\n",
    "\n",
    "ablator.execute_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai]",
   "language": "python",
   "name": "conda-env-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
